{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57dbcd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: torchaudio in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: numpy in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4454803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0902a878-f971-4666-bb88-2b020165e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siewla/.pyenv/versions/3.10.6/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import required libs\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "364cb008-0ccd-4a56-9938-29df6bade8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import self-build excel datasets, manually scrapping the articles on different websites \n",
    "# to be categorized into: [0] -> Healthcare, AI, IoT, Blockchain\n",
    "df = pd.read_excel(\"text_summary_datasets_v2.xlsx\")\n",
    "\n",
    "# tokenizer and bert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8df8de-059e-4e34-acdd-035c852774e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/siewla/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/siewla/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/siewla/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk libs\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1fb882a-f61a-4eb7-b261-a8a71e0df9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text pre-processing function\n",
    "def preprocess_text(text):\n",
    "    # case standardization\n",
    "    text = text.lower() # dont care about capitalization yet\n",
    "    \n",
    "    # puntuation removal\n",
    "    text = text.replace('\"', '') # our text consists of multiple sentences, some punctuations are needed\n",
    "\n",
    "    # tokenized text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # stop word removal\n",
    "    # new_tokens = []\n",
    "    # for token in tokens:\n",
    "    #    if token.lower() not in stop_words:\n",
    "    #        new_tokens.append(token)\n",
    "    '''\n",
    "    original:\n",
    "    ['the', 'diagnosis', 'of', 'v', '##kh', 'followed', 'revised', 'diagnostic', 'criteria', 'by', 'the', 'internation', ...]\n",
    "    remove stop words:\n",
    "    ['diagnosis', 'v', '##kh', 'followed', 'revised', 'diagnostic', 'criteria', 'international', ...]\n",
    "    Thus don't remove stop words, it might lead to poor BERT semantic understand. \n",
    "    '''\n",
    "\n",
    "    # lemmatizer and stemmer\n",
    "    # Lemmatization and Stemming\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # print(tokens)\n",
    "    # print(lemmatized_tokens)\n",
    "    # print(stemmed_tokens)\n",
    "    '''\n",
    "    lemmatized:\n",
    "    ['the', 'diagnosis', 'of', 'v', '##kh', 'followed', 'revised', 'diagnostic', 'criterion', 'by', 'the', 'international', ...]\n",
    "    stemmed:\n",
    "    ['the', 'diagnosi', 'of', 'v', '##kh', 'follow', 'revis', 'diagnost', 'criteria', 'by', 'the', 'intern', ...]\n",
    "    Stemmed is bad here, choose lemmatizer over stemmer.\n",
    "    '''\n",
    "\n",
    "    # change tokens back to senteces\n",
    "    def detokenize(tokens):\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] += token[2:]\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        text = \" \".join(new_tokens)\n",
    "        text = re.sub(r'\\s([?.!,\\'-](?:\\s|$))', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "    text = detokenize(lemmatized_tokens)\n",
    "\n",
    "    # capitalize first alphabet of each sentence\n",
    "    text = re.sub(r\"(^|[.!?]\\s+)(\\w+)\", lambda match: match.group(1) + match.group(2).capitalize(), text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b0150a-9373-49ef-b108-c3213aafa8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "'''\n",
    "choose BERT to get text semantic meaning to be used for classification and clustering. This is more advanced than keywords counting.\n",
    "'''\n",
    "def toBert(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    outputs = bert_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    return outputs.last_hidden_state[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9207dc-b332-4756-b201-5fa63036ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty data\n",
    "data = {\n",
    "    \"Index\": [],\n",
    "    \"Category\": [],\n",
    "    **{f\"dim_{it+1}\": [] for it in range(768)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d5cf073-e820-48ab-b912-b30f9fab5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, row in df.iterrows():\n",
    "    data[\"Index\"].append(row[\"Index\"])\n",
    "    data[\"Category\"].append(row[\"Category\"])\n",
    "    text = str(row[\"Summary\"])\n",
    "    #print(preprocess_text(text))\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    outputs = toBert(text)\n",
    "\n",
    "    for it in range(768):\n",
    "        data[f\"dim_{it+1}\"].append(outputs[it].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508959fd-dedb-4009-a896-9d504c68d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(data)\n",
    "\n",
    "new_df.to_excel(\"training_data_v2.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0902a878-f971-4666-bb88-2b020165e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\huggingman\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "D:\\anaconda3\\envs\\huggingman\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# import required libs\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "364cb008-0ccd-4a56-9938-29df6bade8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\huggingman\\Lib\\site-packages\\huggingface_hub-0.23.3-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    }
   ],
   "source": [
    "# import self-build excel datasets, manually scrapping the articles on different websites \n",
    "# to be categorized into: [0] -> Healthcare, AI, IoT, Blockchain\n",
    "df = pd.read_excel(\"text_summary_datasets_v2.xlsx\")\n",
    "\n",
    "# tokenizer and bert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8df8de-059e-4e34-acdd-035c852774e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk libs\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1fb882a-f61a-4eb7-b261-a8a71e0df9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text pre-processing function\n",
    "def preprocess_text(text):\n",
    "    # case standardization\n",
    "    text = text.lower() # dont care about capitalization yet\n",
    "    \n",
    "    # puntuation removal\n",
    "    text = text.replace('\"', '') # our text consists of multiple sentences, some punctuations are needed\n",
    "\n",
    "    # tokenized text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    ## stop word removal\n",
    "    #new_tokens = []\n",
    "    #for token in tokens:\n",
    "    #    if token.lower() not in stop_words:\n",
    "    #        new_tokens.append(token)\n",
    "    '''\n",
    "    original:\n",
    "    ['the', 'diagnosis', 'of', 'v', '##kh', 'followed', 'revised', 'diagnostic', 'criteria', 'by', 'the', 'internation', ...]\n",
    "    remove stop words:\n",
    "    ['diagnosis', 'v', '##kh', 'followed', 'revised', 'diagnostic', 'criteria', 'international', ...]\n",
    "    Thus don't remove stop words, it might lead to poor BERT semantic understand. \n",
    "    '''\n",
    "\n",
    "    # lemmatizer and stemmer\n",
    "    # Lemmatization and Stemming\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    #stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    #print(tokens)\n",
    "    #print(lemmatized_tokens)\n",
    "    #print(stemmed_tokens)\n",
    "    '''\n",
    "    lemmatized:\n",
    "    ['the', 'diagnosis', 'of', 'v', '##kh', 'followed', 'revised', 'diagnostic', 'criterion', 'by', 'the', 'international', ...]\n",
    "    stemmed:\n",
    "    ['the', 'diagnosi', 'of', 'v', '##kh', 'follow', 'revis', 'diagnost', 'criteria', 'by', 'the', 'intern', ...]\n",
    "    Stemmed is bad here, choose lemmatizer over stemmer.\n",
    "    '''\n",
    "\n",
    "    # change tokens back to senteces\n",
    "    def detokenize(tokens):\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] += token[2:]\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        text = \" \".join(new_tokens)\n",
    "        text = re.sub(r'\\s([?.!,\\'-](?:\\s|$))', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "    text = detokenize(lemmatized_tokens)\n",
    "\n",
    "    # capitalize first alphabet of each sentence\n",
    "    text = re.sub(r\"(^|[.!?]\\s+)(\\w+)\", lambda match: match.group(1) + match.group(2).capitalize(), text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b0150a-9373-49ef-b108-c3213aafa8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "'''\n",
    "choose BERT to get text semantic meaning to be used for classification and clustering. This is more advanced than keywords counting.\n",
    "'''\n",
    "def toBert(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    outputs = bert_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    return outputs.last_hidden_state[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a9207dc-b332-4756-b201-5fa63036ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty data\n",
    "data = {\n",
    "    \"Index\": [],\n",
    "    \"Category\": [],\n",
    "    **{f\"dim_{it+1}\": [] for it in range(768)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5cf073-e820-48ab-b912-b30f9fab5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, row in df.iterrows():\n",
    "    data[\"Index\"].append(row[\"Index\"])\n",
    "    data[\"Category\"].append(row[\"Category\"])\n",
    "    text = str(row[\"Summary\"])\n",
    "    #print(preprocess_text(text))\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    outputs = toBert(text)\n",
    "\n",
    "    for it in range(768):\n",
    "        data[f\"dim_{it+1}\"].append(outputs[it].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508959fd-dedb-4009-a896-9d504c68d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(data)\n",
    "\n",
    "new_df.to_excel(\"training_data_v2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618db998-1937-4602-8676-13f0181bfa35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
